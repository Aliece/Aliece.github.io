---
title: Sharding!
first: 数据库设计
layout: default
author:
  name: Aliece TT
  url: http://aliecett.wicp.net
---

##OceanBase

[https://github.com/alibaba/oceanbase/wiki/Oceanbase-0.4.2-%E6%8F%8F%E8%BF%B0_1-%E6%9E%B6%E6%9E%84%E5%88%9D%E6%8E%A2](https://github.com/alibaba/oceanbase/wiki/Oceanbase-0.4.2-%E6%8F%8F%E8%BF%B0_1-%E6%9E%B6%E6%9E%84%E5%88%9D%E6%8E%A2)

OceanBase的目标是支持数百TB的数据量以及数十万TPS、数百万QPS的访问量。无论是数据量还是访问量，即使采用非常昂贵的小型机甚至是大型机，单台关系数据库系统都无法承受。当前常见的处理方法主要有以下两种：

一种常见的做法是根据业务特点对数据库进行水平拆分。根据某个业务字段，通常取用户编号，哈希后取模，根据取模的结果将数据分布到不同的数据库服务器上，客户端请求通过数据库中间层路由到不同的分区。这种方式目前还存在一定的弊端：

第一，数据和负载增加后添加机器的操作比较复杂，需要人工介入。

第二，有些范围查询需要访问几乎所有的分区，例如，按照用户编号分区，查询收藏了一个商品的所有用户需要访问所有的分区。

第三，目前广泛使用的关系数据库存储引擎都是针对机械硬盘的特点设计的，不能够完全发挥新硬件（SSD）的能力。

另外一种做法是参考分布式表格系统的做法。例如Google Bigtable系统，将大表划分为几万、几十万甚至几百万个子表，子表之间按照主键有序，如果某台服务器发生故障，它上面服务的数据能够在很短的时间内自动迁移到集群中所有的其它服务器。这种方式解决了可扩展性的问题，少量突发的服务器故障或者增加服务器对使用者基本是透明的，能够轻松应对促销或者热点事件等突发流量增长。另外，由于子表是按照主键有序分布的，很好地解决了范围查询的问题。

万事有其利必有一弊，分布式表格系统虽然解决了可扩展性问题，但往往无法支持事务。例如Bigtable只支持单行事务，针对同一个user_id下的多条记录的操作都无法保证原子性。而OceanBase希望能够支持跨行跨表事务，这样使用起来会比较方便。

最直接的做法是在Bigtable开源实现（如HBase或者Hypertable）的基础上引入两阶段提交（Two-phase Commit）协议支持分布式事务，这种思路在Google的Percolator系统中得到了体现。然而，Percolator系统中事务的平均响应时间达到2~5秒，只能应用在类似网页建库这样的半线上业务中。另外，Bigtable的开源实现也不够成熟，单台服务器能够支持的数据量有限，单个请求的最大响应时间很难得到保证，机器故障等异常处理机制也有很多比较严重的问题。总体上看，这种做法的工作量和难度超出了项目组的承受能力，因此，我们需要根据业务特点做一些定制。

通过分析，我们发现，虽然在线业务的数据量十分庞大，例如几十亿条、上百亿条甚至更多记录，但最近一段时间（例如一天）的修改量往往并不多，通常不超过几千万条到几亿条，因此，OceanBase决定采用单台更新服务器来记录最近一段时间的修改增量，称为增量数据；而以前的数据保持不变，称为基准数据。基准数据以类似分布式文件系统的方式存储于多台基准数据服务器中，每次查询都需要把基准数据和增量数据融合后返回给客户端。因此，写事务都将集中在单台更新服务器上，避免了复杂的分布式事务，高效地实现了跨行跨表事务。另外，更新服务器上的修改增量能够定期分发到多台基准数据服务器中，避免成为瓶颈，实现了良好的扩展性。

当然，单台更新服务器的处理能力总是有一定的限制。因此，更新服务器的硬件配置较高，如内存较大、网卡及CPU较好。另外，最近一段时间的更新操作往往总是能够存放在内存中，所以在软件层面也针对这种场景做了大量的优化。

##cobar(cobarclient)


##openclouddb


##Tddl